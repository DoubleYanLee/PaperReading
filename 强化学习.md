## 深入浅出强化学习原理

### 马尔科夫决策过程

#### 马尔科夫性

* 该系统的下一个状态$S_{t+1}$只与当前状态$s_t$有关
* 如果随机变量中的每个状态都是马尔科夫的，则此随机过程为马尔科夫随机过程。

#### 马尔科夫过程

* 马尔科夫过程是一个二元组(S,P),S是有限状态集合，P是状态转移概率$$\left[\begin{matrix}   P_{11} & P_{12} & P_{13}\\  P_{21} & P_{22} &P_{23}\\  P_{31} & P_{32} & P_{33}  \end{matrix}  \right] $$（思考上课睡觉那个图）
* 此时不考虑动作和奖励，将动作和回报考虑在内的马尔科夫过程称为马尔科夫决策过程。

#### 马尔科夫决策过程

* 马尔科夫决策过程由元组$(S,A,P,R,\gamma)$描述。P为状态转移概率，此时包含动作了（思考状态s1 s2 那个图，玩  睡觉 学习变成了边上的权值，也就是动作） $\gamma$为折扣因子，用来计算累计回报

* 强化学习的目标是：给定一个马尔科夫决策过程，寻找最优策略。即policy $\pi$ ,这里的$\pi$是一个条件概率分布。举个例子：学生1的策略$\pi(玩｜s1) = 0.8$  学生2的策略$\pi(玩｜s1) = 0.3$  即学生1在状态s1时玩的概率为0.8，学生2则只为0.3。则学生1更爱玩，而学生2更不爱玩。而有了这个策略之后，就会得到不一样的回报。强化学习就是想训练出一个好的策略，来得到较大的总回报。

  >$\pi(a|s) = p[A_t = a | S_t = s]$

* 状态值函数

  * accumulated reward

    $G_t = R_{t+1}+\gamma R_{t+2}+... = \sum_{k=0}^\infty \gamma^k R_{t+k+1}$

  * 当agent采用policy $\pi$ 时，accumulated reward服从一个分布。将accumulated reward在状态s处的期望值定义为 state-value 函数

    $V_\pi(s) = E_\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t = s]$

  * 状态 - 行为值函数为：

    $q_\pi(s,a) = E_\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t = s,A_t = a]$

* 状态值函数和状态 - 行为值函数的贝尔曼方程 （这里需要自己手推一下，没看懂。。）

  * $V(S_t) = E[R_{t+1} + \gamma V(S_{t+1})]$
  * $q_\pi(s,a) = E_\pi[R_{t+1}+ \gamma q(S_{t+1},A_{t+1})|S_t = s, A_t = a]$

  由此可以得到最优状态值函数和最优状态 - 动作值函数

* 最常用的概率分布，即最常用的随机策略
  * 贪婪策略：是一个确定性的策略，即只有在使动作值函数最大的动作处概率为1，选其他动作的概率为0
  * $\epsilon-greedy$ 策略：是一个随机策略，在使动作值函数最大的动作处概率为 $1-\epsilon + \frac{\epsilon}{|A(s)|}$,而其他动作的概率为等概率，都为$\frac{\epsilon}{|A(s)|}$   其平衡了exploitation和exploration
  * 高斯策略：$\pi_{\theta} = \mu_{\theta} + \epsilon , \epsilon\in N(0,\sigma^2)$ $\mu_{\theta}$为确定性部分, $\epsilon$为零均值的高斯随机噪声,其也平衡了exploitation(利用部分)和exploration  高斯策略在连续系统的强化学习中应用广泛。
  * 玻尔兹曼分布： 对于动作空间时离散的或者动作空间并不大的情况，采用玻尔兹曼分布作为随机策略 $\pi(a|s,\theta) = \frac{exp(Q(s,a,\theta))}{\sum_b exp(h(s,b,\theta))}$ 含义是：动作值函数大的动作被选中的概率大

* 随机策略：通常用符号$\pi$来表示，它指给定状态s时动作集上的一个分布。

### 基于模型的动态规划方法

* accumulated reward $R(\tau)$是个随机变量，由于随机变量无法进行优化，无法作为目标函数，所以使用随机变量的期望作为目标函数。即$\int R(\tau)p_\pi(\tau)d\tau$ (这里我认为$p_\pi(\tau)$是$R(\tau)$的概率密度函数)

  ![pic1](/Users/yannie/Desktop/PaperReading/pic/pic1.png)

* 在马尔科夫决策过程中，根据转移概率p是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法。

* 策略迭代算法包括策略评估和策略改善两个步骤。策略评估中，给定策略，通过数值迭代算法不断计算该策略下每个状态的值函数，利用该值函数和贪婪策略得到新的策略。

* 解决最优控制问题通常有三种思路：变分法原理，庞特里亚金最大值原理和动态规划方法。





### 基于蒙特卡罗的强化学习方法

* 无模型的强化学习算法主要包括蒙特卡罗法和时间差分法。

* 状态值函数和行为值函数的计算实际上是计算返回值的期望。 动态规划方法是利用模型计算期望。在没有模型的时候，我们可以采用蒙特卡罗的方法计算该期望，即利用随机样本估计期望。

* 在计算值函数的时候，蒙塔卡罗方法是利用经验平均代替随机变量的期望。  经验即指每一个episode，平均指的是求平均值。利用蒙特卡罗方法求状态s处的值函数时，又可以分为第一次访问和每次访问蒙特卡罗方法。

* 在蒙特卡罗方法中必须采用一定的方法保证每个状态都能被访问到。方法之一是：探索性初始化：指每个状态都有一定的机率作为初始状态。

* 根据探索策略和评估策略是否是同一个策略，蒙塔卡罗方法又分为 on-policy 和 off-policy两种方法。

* 同策略(on-policy)指产生数据的策略与评估和要改善的策略是同一个策略。

* 异策略(off-policy)则与之相反，其可以保证充分的探索性。如用于评估和改善的策略$\pi$是贪婪策略，用于产生数据的探索性策略$\mu$是$\epsilon-soft $策略。 这两种策略必须满足条件： 行动策略$\mu$产生的行为覆盖或包括目标策略$\pi$产生的行为。

* 利用行动策略产生的数据评估目标策略需要利用**重要性采样**方法。

* 重要性采样：原随机变量z的分布非常复杂时，无法利用解析的方法产生用于逼近期望的样本时2，就可以选用一个概率分布很简单，很容易产生样本的概率分布$q(z)$,则原期望可变为：

  $$E[f] = \int f(z)p(z)dz = \int f(z)\frac{p(z)}{q(z)}q(z) dz \approx \frac{1}{N}\sum_n \frac{p(z^n)}{q(z^n)f(z^n)}, z^n \approx q(z)$$

* 基于重要性采样的积分估计为无偏估计（即估计的期望值等于真实的期望值）但基于重要性采样的积分估计的方差无无穷大，这是因为原来的被积函数乘了一个重要性权重$W^n = \frac{p(z^n)}{q(z^n)}$。则$E[f] = \frac{1}{N}\sum w^n f(z^n)$

   减少重要性采样积分方差的方法是：采用加权重要性采样。 $E[f] \approx \sum_{n=1}^N \frac{w^n}{\sum_{m=1}^N w^m}f(z^n)$

### 基于时间差分的强化学习方法

* 时间差分法(Temporal-Difference TD方法)

  * 使用动态规划方法计算值函数利用的是bootstrapping方法：即利用后继状态的值函数估计当前值函数。此处后继状态是由模型公式$p(s',r|S_t, a)$计算得到的。当没有模型时，后继状态无法全部得到，就只能通过试验和采样的方法每次试验得到一个后继状态$s'$ 。
  * 一次试验要等终止状态出现才结束。所以对比动态规划方法，蒙特卡罗方法方法需要每次等到试验结束，所以学习速度慢，学习效率不高。所以就设想能否借鉴bootstrapping方法，在试验未结束时就估计当前的值函数呢？ -> TD方法
  * 值函数的公式更新为：

  $V(S_t) \leftarrow V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t)) $

* 时间差分方法包括：同策略的Sarsa方法和异策略的Q-learning方法。Sarsa方法行动策略和评估策略都是$\epsilon-greedy$策略，而Q-learning行动策略采用的是$\epsilon-greedy$策略，而目标策略为贪婪策略。

* TD方法除了Sarsa方法和Q-learning方法，还包括$TD(\lambda)$方法

### 基于值函数逼近的强化学习方法

* 前面的这三种方法有一个前提是：状态空间和动作空间是离散的，且不能太大。其基本步骤都是先评估值函数，再利用值函数改变当前的策略。⚠️ 此时值函数是一个表格。对于状态值函数，其索引是状态；对于行为值函数，其索引是状态-行为对。值函数的迭代更新实际上就是这张表的迭代更新。当状态空间维度很大，或者为连续空间的时候，此时值函数无法用一个表格来表示，就需要利用值函数逼近的方法来表示值函数。(策略迭代/值迭代)

* 函数逼近的方法可以分为参数化逼近和非参数化逼近。参数化逼近指的是：值函数可以由一组参数$\theta$来近似。

* 蒙特卡罗方法，值函数更新公式为：

  ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Cleft%28s%2Ca%5Cright%29%5Cgets+Q%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cleft%28G_t-Q%5Cleft%28s%2Ca%5Cright%29%5Cright%29+%5C%5D) (5.1)

  TD方法值函数更新公式为：

  ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Cleft%28s%2Ca%5Cright%29%5Cgets+Q%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cleft%5Br%2B%5Cgamma+Q%5Cleft%28s%27%2Ca%27%5Cright%29-Q%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D) (5.2)

  ![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)方法值函数更新公式为：

  ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Cleft%28s%2Ca%5Cright%29%5Cgets+Q%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cleft%5BG_%7Bt%7D%5E%7B%5Clambda%7D-Q%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D) (5.3)

  从表格型值函数的更新过程，我们不难总结出不管是蒙特卡罗方法还是TD方法，都是朝着一个目标值更新的，这个目标值在蒙特卡罗方法中是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_t+%5C%5D) ，在TD方法中是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+r%2B%5Cgamma+Q%5Cleft%28s%27%2Ca%27%5Cright%29+%5C%5D) ，在![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+TD%5Cleft%28%5Clambda%5Cright%29+%5C%5D)中是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_%7Bt%7D%5E%7B%5Clambda%7D+%5C%5D) 。

* 函数逼近![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cupsilon%7D%5Cleft%28s%2C%5Ctheta%5Cright%29+%5C%5D) 的过程是一个监督学习的过程，其数据和标签对为：![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28S_t%2CU_t%5Cright%29), 其中![[公式]](https://www.zhihu.com/equation?tex=U_t)
  等价于蒙特卡罗方法中的![[公式]](https://www.zhihu.com/equation?tex=G_t)，TD方法中的 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+r%2B%5Cgamma+Q%5Cleft%28s%27%2Ca%27%5Cright%29+%5C%5D) ，以及 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+TD%5Cleft%28%5Clambda%5Cright%29+%5C%5D) 中的 ![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Clambda%7D)。

  训练的目标函数为：

  ![[公式]](https://www.zhihu.com/equation?tex=argmin_%7B%5Ctheta%7D%5Cleft%28q%5Cleft%28s%2Ca%5Cright%29-%5Chat%7Bq%7D%5Cleft%28s%2Ca%2C%5Ctheta%5Cright%29%5Cright%29%5E2+) (5.4)

* 1）表格型强化学习进行值函数更新时，只有当前状态![[公式]](https://www.zhihu.com/equation?tex=S_t)处的值函数在改变，其他地方的值函数不发生改变。

  2）值函数逼近方法进行值函数更新时，因此更新的是参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)，而估计的值函数为![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cupsilon%7D%5Cleft%28s%2C%5Ctheta%5Cright%29)，所以当参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)发生改变时，任意状态处的值函数都会发生改变。

  值函数更新可分为增量式学习方法和批学习方法。**我们先介绍增量式学习方法。随机梯度下降法是最常用的增量式学习方法。**

* 

























